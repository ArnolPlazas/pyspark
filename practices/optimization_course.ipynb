{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark optimization\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as a list of tuples\n",
    "data = [('James', 34), ('Anna', 20), ('Lee', 30)]\n",
    "\n",
    "columns = ['Name', 'Age']\n",
    "df = spark.createDataFrame(data, schema=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType\n",
    "\n",
    "# Data as a list of tuples\n",
    "data = [(\"James\", 34), (\"Anna\", 20), (\"Lee\", 30)]\n",
    "\n",
    "\n",
    "# Use Schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as a list of tuples\n",
    "data = [(\"James\", 34), (\"Anna\", 20), (\"Lee\", 30)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|James| 34|\n",
      "| Anna| 20|\n",
      "|  Lee| 30|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Show Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n",
      "StructType([StructField('Name', StringType(), True), StructField('Age', IntegerType(), True)])\n",
      "['Name', 'Age']\n",
      "+-------+----+-----------------+\n",
      "|summary|Name|              Age|\n",
      "+-------+----+-----------------+\n",
      "|  count|   3|                3|\n",
      "|   mean|NULL|             28.0|\n",
      "| stddev|NULL|7.211102550927978|\n",
      "|    min|Anna|               20|\n",
      "|    max| Lee|               34|\n",
      "+-------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print(df.schema)\n",
    "print(df.columns)\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|James|\n",
      "| Anna|\n",
      "|  Lee|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|James|\n",
      "| Anna|\n",
      "|  Lee|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|James|\n",
      "| Anna|\n",
      "|  Lee|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|James|\n",
      "| Anna|\n",
      "|  Lee|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[0]).show()\n",
    "df.select(df.Name).show()\n",
    "df.select(df[\"Name\"]).show()\n",
    "df.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|James| 34|\n",
      "|  Lee| 30|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|James| 34|\n",
      "|  Lee| 30|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|James| 34|\n",
      "|  Lee| 30|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|James| 34|\n",
      "|  Lee| 30|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[1] > 25).show()\n",
    "df.filter(df.Age > 25).show()\n",
    "df.filter(df[\"Age\"] > 25).show()\n",
    "df.filter('Age > 25').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write Dataframe to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a JSON file\n",
    "df.write.json(\"./output/test.json\")\n",
    "\n",
    "# Writing a Parquet file\n",
    "df.write.parquet(\"./output/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reading file to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age| Name|\n",
      "+---+-----+\n",
      "| 34|James|\n",
      "| 20| Anna|\n",
      "| 30|  Lee|\n",
      "+---+-----+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|James| 34|\n",
      "| Anna| 20|\n",
      "|  Lee| 30|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading a JSON file\n",
    "df_json = spark.read.json(\"./output/test.json\")\n",
    "df_json.show()\n",
    "# Reading a Parquet file\n",
    "df_parquet = spark.read.parquet(\"./output/test.parquet\")\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add new Column with new Complex DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "| Name|Age| NameAndAge|\n",
      "+-----+---+-----------+\n",
      "|James| 34|{James, 34}|\n",
      "| Anna| 20| {Anna, 20}|\n",
      "|  Lee| 30|  {Lee, 30}|\n",
      "+-----+---+-----------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- NameAndAge: struct (nullable = false)\n",
      " |    |-- Name: string (nullable = true)\n",
      " |    |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "df2 = df.withColumn(\"NameAndAge\", struct(df.Name, df.Age))\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query: Grouping and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "data = [(\"James\", \"Sales\", 3000), \n",
    "        (\"Michael\", \"Sales\", 4600), \n",
    "        (\"Robert\", \"Sales\", 4100),\n",
    "        (\"Maria\", \"Finance\", 3000),\n",
    "        (\"James\", \"Sales\", 3000),\n",
    "        (\"Scott\", \"Finance\", 3300),\n",
    "        (\"Jen\", \"Finance\", 3900),\n",
    "        (\"Jeff\", \"Marketing\", 3000),\n",
    "        (\"Kumar\", \"Marketing\", 2000),\n",
    "        (\"Saif\", \"Sales\", 4100)]\n",
    "columns = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    5|\n",
      "|   Finance|    3|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "grouped_df = df.groupBy(\"department\").count()\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max(), min(), avg(), and sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|       4600|\n",
      "|   Finance|       3900|\n",
      "| Marketing|       3000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform GroupBy and Max\n",
    "max_df = df.groupBy(\"department\").max(\"salary\").alias(\"max_salary\")\n",
    "max_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|       3000|\n",
      "|   Finance|       3000|\n",
      "| Marketing|       2000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform GroupBy and Max\n",
    "max_df = df.groupBy(\"department\").min(\"salary\").alias(\"min_salary\")\n",
    "max_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|avg(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|     3760.0|\n",
      "|   Finance|     3400.0|\n",
      "| Marketing|     2500.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform GroupBy and avg\n",
    "max_df = df.groupBy(\"department\").avg(\"salary\").alias(\"avg_salary\")\n",
    "max_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      18800|\n",
      "|   Finance|      10200|\n",
      "| Marketing|       5000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform GroupBy and Max\n",
    "max_df = df.groupBy(\"department\").sum(\"salary\").alias(\"sum_salary\")\n",
    "max_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. agg() + F.max(), F.count() etc…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+----------+------------+--------------+\n",
      "|department|count|max_salary|min_salary|total_salary|average_salary|\n",
      "+----------+-----+----------+----------+------------+--------------+\n",
      "|     Sales|    5|      4600|      3000|       18800|        3760.0|\n",
      "|   Finance|    3|      3900|      3000|       10200|        3400.0|\n",
      "| Marketing|    2|      3000|      2000|        5000|        2500.0|\n",
      "+----------+-----+----------+----------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Perform multiple aggregations\n",
    "agg_df = df.groupBy(\"department\").agg(\n",
    "    F.count(\"salary\").alias(\"count\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.sum(\"salary\").alias(\"total_salary\"),\n",
    "    F.avg(\"salary\").alias(\"average_salary\")\n",
    ")\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. agg() + collect_list() and collect_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------+-------------------+\n",
      "|department|collect_list(salary)          |collect_set(salary)|\n",
      "+----------+------------------------------+-------------------+\n",
      "|Sales     |[3000, 4600, 4100, 3000, 4100]|[4600, 3000, 4100] |\n",
      "|Finance   |[3000, 3300, 3900]            |[3900, 3000, 3300] |\n",
      "|Marketing |[3000, 2000]                  |[3000, 2000]       |\n",
      "+----------+------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "\n",
    "# Perform GroupBy and collect list\n",
    "collected_list_df = df.groupBy(\"department\").agg(\n",
    "  collect_list(\"salary\"),\n",
    "  collect_set(\"salary\") # drop duplicates\n",
    ")\n",
    "collected_list_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. agg() + User-Defined Aggregation Functions (UDAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|department|average_salary|\n",
      "+----------+--------------+\n",
      "|   Finance|        3400.0|\n",
      "| Marketing|        2500.0|\n",
      "|     Sales|        3760.0|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def mean_salary(s: pd.Series) -> float:\n",
    " return s.mean()\n",
    "\n",
    "udaf_df = df.groupBy(\"department\").agg(\n",
    "  mean_salary(df[\"salary\"]).alias(\"average_salary\")\n",
    ")\n",
    "udaf_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. agg() + Complex Conditions: when()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|sum_high_salaries|\n",
      "+----------+-----------------+\n",
      "|     Sales|            12800|\n",
      "|   Finance|             7200|\n",
      "| Marketing|                0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, when, col\n",
    "\n",
    "# Conditional Aggregation\n",
    "conditional_agg_df = df.groupBy(\"department\").agg(\n",
    "    # sum(when(df[\"salary\"] > 3000, df[\"salary\"]).otherwise(0)).alias(\"sum_high_salaries\")\n",
    "    sum(when(col(\"salary\") > 3000, col(\"salary\")).otherwise(0)).alias(\"sum_high_salaries\")\n",
    "\n",
    ")\n",
    "conditional_agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using RDD Map Function with GroupBy after agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sales', 4600), ('Finance', 3900), ('Marketing', 3000)]\n",
      "+----------+----------+\n",
      "|department|max_salary|\n",
      "+----------+----------+\n",
      "|     Sales|      4600|\n",
      "|   Finance|      3900|\n",
      "| Marketing|      3000|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying map operation after GroupBy\n",
    "result_rdd = df.groupBy(\"department\").agg(\n",
    "  collect_list(\"salary\")\n",
    ").rdd.map(\n",
    "  lambda x: (x[0], max(x[1]))\n",
    ")\n",
    "print(result_rdd.collect())\n",
    "\n",
    "result_df = spark.createDataFrame(result_rdd, [\"department\", \"max_salary\"])\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query: Something Else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. rollup() and cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 211:=============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+\n",
      "|department|employee_name|sum(salary)|\n",
      "+----------+-------------+-----------+\n",
      "|     Sales|        James|       6000|\n",
      "|      NULL|         NULL|      34000|\n",
      "|     Sales|         NULL|      18800|\n",
      "|     Sales|      Michael|       4600|\n",
      "|     Sales|       Robert|       4100|\n",
      "|   Finance|         NULL|      10200|\n",
      "|   Finance|        Maria|       3000|\n",
      "|   Finance|        Scott|       3300|\n",
      "|   Finance|          Jen|       3900|\n",
      "| Marketing|         NULL|       5000|\n",
      "| Marketing|         Jeff|       3000|\n",
      "| Marketing|        Kumar|       2000|\n",
      "|     Sales|         Saif|       4100|\n",
      "+----------+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Rollup() generates a multidimensional aggregation and provides a hierarchical summary, akin to subtotals in Excel.\n",
    "rollup_df = df.rollup(\"department\", \"employee_name\").sum(\"salary\")\n",
    "rollup_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+\n",
      "|department|employee_name|sum(salary)|\n",
      "+----------+-------------+-----------+\n",
      "|      NULL|        James|       6000|\n",
      "|     Sales|        James|       6000|\n",
      "|      NULL|         NULL|      34000|\n",
      "|     Sales|         NULL|      18800|\n",
      "|      NULL|      Michael|       4600|\n",
      "|     Sales|      Michael|       4600|\n",
      "|     Sales|       Robert|       4100|\n",
      "|      NULL|       Robert|       4100|\n",
      "|   Finance|         NULL|      10200|\n",
      "|   Finance|        Maria|       3000|\n",
      "|      NULL|        Maria|       3000|\n",
      "|      NULL|        Scott|       3300|\n",
      "|   Finance|        Scott|       3300|\n",
      "|      NULL|          Jen|       3900|\n",
      "|   Finance|          Jen|       3900|\n",
      "| Marketing|         NULL|       5000|\n",
      "| Marketing|         Jeff|       3000|\n",
      "|      NULL|         Jeff|       3000|\n",
      "| Marketing|        Kumar|       2000|\n",
      "|      NULL|        Kumar|       2000|\n",
      "+----------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cube(): Cube generates a multidimensional aggregation and provides insights across multiple combinations of the specified grouping columns.\n",
    "cube_df = df.cube(\"department\", \"employee_name\").sum(\"salary\")\n",
    "cube_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. groupBy() + pivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+----+-----+-----+-------+------+----+-----+\n",
      "|department|James|Jeff| Jen|Kumar|Maria|Michael|Robert|Saif|Scott|\n",
      "+----------+-----+----+----+-----+-----+-------+------+----+-----+\n",
      "|     Sales| 6000|NULL|NULL| NULL| NULL|   4600|  4100|4100| NULL|\n",
      "|   Finance| NULL|NULL|3900| NULL| 3000|   NULL|  NULL|NULL| 3300|\n",
      "| Marketing| NULL|3000|NULL| 2000| NULL|   NULL|  NULL|NULL| NULL|\n",
      "+----------+-----+----+----+-----+-----+-------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = df.groupBy('department').pivot('employee_name').sum('salary')\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Window Functions: partitionBy() + row_number()/rank().over(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         2|\n",
      "|       Robert|     Sales|  4100|         3|\n",
      "|         Saif|     Sales|  4100|         4|\n",
      "|      Michael|     Sales|  4600|         5|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").asc())\n",
    "df_with_row_number = df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "df_with_row_number.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|          Jen|   Finance|  3900|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|        Maria|   Finance|  3000|   3|\n",
      "|         Jeff| Marketing|  3000|   1|\n",
      "|        Kumar| Marketing|  2000|   2|\n",
      "|      Michael|     Sales|  4600|   1|\n",
      "|       Robert|     Sales|  4100|   2|\n",
      "|         Saif|     Sales|  4100|   2|\n",
      "|        James|     Sales|  3000|   4|\n",
      "|        James|     Sales|  3000|   4|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df_with_rank = df.withColumn(\"rank\", rank().over(windowSpec))\n",
    "df_with_rank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations I: Lessen Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cache a DataFrame when it is accessed multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Appropriate File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-4a511f89-6f29-419b-a434-e7ecbfae61b4-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# compressed files can save the files I/O and memory\n",
    "output_dir = \"./output/output.parquet\"\n",
    "df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_dir)\n",
    "\n",
    "# Find the single generated parquet file and rename it\n",
    "parquet_file = [f for f in os.listdir(output_dir) if f.endswith('.parquet')][0]\n",
    "print(parquet_file)\n",
    "shutil.move(os.path.join(output_dir, parquet_file), './output/out.parquet')\n",
    "\n",
    "# Remove the folder created by Spark\n",
    "shutil.rmtree(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Specifying Schema Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Arnol| 34|\n",
      "|  2| Camila| 35|\n",
      "|  3|Mathieu|  4|\n",
      "|  4|  Maite|  3|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "df = spark.read.schema(schema).csv(\"../data/users.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting Columns Early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "| Arnol| 34|\n",
      "|Camila| 35|\n",
      "+------+---+\n",
      "\n",
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "| Arnol| 34|\n",
      "|Camila| 35|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the necessary columns early in your data processing pipeline to reduce memory usage.\n",
    "df.select(\"name\", \"age\").filter(\"age >= 10\").show()\n",
    "df.select(\"name\", \"age\").filter(df.age >= 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply filters Early, especially before Joins and Aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Arnol| 34|\n",
      "|  2| Camila| 35|\n",
      "|  3|Mathieu|  4|\n",
      "|  4|  Maite|  3|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"id_user\", IntegerType(), True),\n",
    "    StructField(\"sport\", StringType(), True)\n",
    "])\n",
    "df_sport = spark.read.schema(schema).csv(\"../data/user_sports.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+-------+----------+\n",
      "| id|  name|age| id|id_user|     sport|\n",
      "+---+------+---+---+-------+----------+\n",
      "|  1| Arnol| 34|  1|      1|  football|\n",
      "|  2|Camila| 35|  3|      2|basketball|\n",
      "+---+------+---+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"age > 10\").join(df_sport, df[\"id\"] == df_sport[\"id_user\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using limit() to avoid Collecting Large DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=3, name='Mathieu', age=4)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoid using collect() on large datasets to prevent out of memory errors.\n",
    "df.filter(\"age < 10\").limit(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using spark.sql(): Catalyst optimizer for Complex Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leverage Spark SQL for complex queries, which might be more readable and can benefit from Catalyst optimizer.\n",
    "df.createOrReplaceTempView(\"table\") \n",
    "spark.sql(\"SELECT count(*) FROM table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using RDD: Aggregate with ReduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Arnol'), (2, 'Camila'), (3, 'Mathieu'), (4, 'Maite')]\n",
      "[(1, 'Arnol'), (2, 'Camila'), (3, 'Mathieu'), (4, 'Maite')]\n",
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "|  1|  Arnol|\n",
      "|  2| Camila|\n",
      "|  3|Mathieu|\n",
      "|  4|  Maite|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = df.rdd.map(lambda x: (x[0], x[1]))\n",
    "print(rdd.collect())\n",
    "reduced = rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(reduced.collect())\n",
    "reduced.toDF([\"key\", \"value\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations II: No Partitions, No Parallelized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Partitioning Tables: partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+---------+--------------------+\n",
      "| id| nombre_juego|annio|temporada|              ciudad|\n",
      "+---+-------------+-----+---------+--------------------+\n",
      "|  1|  1896 Verano| 1896|   Verano|              Athina|\n",
      "|  2|  1900 Verano| 1900|   Verano|               Paris|\n",
      "|  3|  1904 Verano| 1904|   Verano|           St. Louis|\n",
      "|  4|  1906 Verano| 1906|   Verano|              Athina|\n",
      "|  5|  1908 Verano| 1908|   Verano|              London|\n",
      "|  6|  1912 Verano| 1912|   Verano|           Stockholm|\n",
      "|  7|  1920 Verano| 1920|   Verano|           Antwerpen|\n",
      "|  8|1924 Invierno| 1924| Invierno|            Chamonix|\n",
      "|  9|  1924 Verano| 1924|   Verano|               Paris|\n",
      "| 10|1928 Invierno| 1928| Invierno|        Sankt Moritz|\n",
      "| 11|  1928 Verano| 1928|   Verano|           Amsterdam|\n",
      "| 12|1932 Invierno| 1932| Invierno|         Lake Placid|\n",
      "| 13|  1932 Verano| 1932|   Verano|         Los Angeles|\n",
      "| 14|1936 Invierno| 1936| Invierno|Garmisch-Partenki...|\n",
      "| 15|  1936 Verano| 1936|   Verano|              Berlin|\n",
      "| 16|1948 Invierno| 1948| Invierno|        Sankt Moritz|\n",
      "| 17|  1948 Verano| 1948|   Verano|              London|\n",
      "| 18|1952 Invierno| 1952| Invierno|                Oslo|\n",
      "| 19|  1952 Verano| 1952|   Verano|            Helsinki|\n",
      "| 20|1956 Invierno| 1956| Invierno|   Cortina d'Ampezzo|\n",
      "+---+-------------+-----+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_games = spark.read.option(\"header\",True).csv(\"../data/juegos.csv\")\n",
    "df_games.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use partitioning when saving DataFrames to disk for faster subsequent reads.\n",
    "df_games.write.partitionBy(\"temporada\", \"annio\").parquet(\"./output/games.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage Skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Data skew happens during a join operation when one or more key values have significantly more data than others.\n",
    "+ For example, if you’re joining on a “customer_id” and most of your transactions belong to a small number of customers, these few keys will end up with a large amount of data compared to other keys. This causes certain tasks (those processing the large keys) to take much longer, resulting in a bottleneck.\n",
    "+ **This solution** : Add a random prefix to keys to manage skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 308:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|         4|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, expr \n",
    "df.withColumn(\"salted_key\", \n",
    "    expr(\"concat(name, '_', (monotonically_increasing_id() % 10))\")\n",
    ").groupBy(\"salted_key\").count().select(sum(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|salted_key|count|\n",
      "+----------+-----+\n",
      "| Mathieu_2|    1|\n",
      "|  Camila_1|    1|\n",
      "|   Arnol_0|    1|\n",
      "|   Maite_3|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How it balance the loading of data?\n",
    "from pyspark.sql.functions import monotonically_increasing_id, expr \n",
    "df.withColumn(\"salted_key\", \n",
    "    expr(\"concat(name, '_', (monotonically_increasing_id() % 10))\")\n",
    ").groupBy(\"salted_key\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations III: Strategies to Minimize Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies to Minimize Shuffling\n",
    "- **Broadcast Variables**\n",
    "  - Use for small datasets to avoid shuffling by broadcasting to all nodes.\n",
    "- **Partition Tuning**\n",
    "  - Adjust the number of partitions to match the scale of the task and data.\n",
    "- **Optimize Transformations**\n",
    "  - Plan operations to minimize wide transformations that require shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling\n",
    "+ Shuffling is a process where data is redistributed **across different PARTITIONS**.\n",
    "+ It involves moving data across executors or even **across MACHINES**.\n",
    "+ It is one of the most expensive operations in terms of network and disk I/O.\n",
    "\n",
    "### Purpose of Shuffling\n",
    "+ **Data Redistribution**: Facilitates wide transformations such as: Joins, GroupBy, Aggregations, and Repartitioning\n",
    "+ **Load Balancing**: Ensures even data and workload distribution across the cluster.\n",
    "+ **Concurrency**: Enhances parallel processing and optimizes resource utilization.\n",
    "+ **Optimizing Data Locality**: Moves data closer to where it will be processed. Reduces network traffic.\n",
    "\n",
    "### Painpoints of Shuffling\n",
    "+ **Resource Intensive**: Consumes significant network bandwidth and disk I/O.\n",
    "+ **Increased Latency**: Adds substantial processing time, especially with large datasets.\n",
    "+ **Potential Bottlenecks**: Can slow down overall system performance if not managed properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use broadcast join to minimize data shuffling when joining a small DataFrame with a large one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+---------+\n",
      "|dept_id|emp_id|name|dept_name|\n",
      "+-------+------+----+---------+\n",
      "|    101|     1|John|       HR|\n",
      "|    102|     2|Jane|Marketing|\n",
      "|    103|     3| Joe|  Finance|\n",
      "|    101|     4|Jill|       HR|\n",
      "+-------+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Create a larger DataFrame for Employees\n",
    "data_employees = [(1, \"John\", 101),\n",
    "                  (2, \"Jane\", 102),\n",
    "                  (3, \"Joe\", 103),\n",
    "                  (4, \"Jill\", 101),\n",
    "                  # Assume many more records exist\n",
    "                  ]\n",
    "columns_employees = [\"emp_id\", \"name\", \"dept_id\"]\n",
    "df_employees = spark.createDataFrame(data_employees, columns_employees)\n",
    "# Create a small DataFrame for Departments\n",
    "data_departments = [(101, \"HR\"),\n",
    "                    (102, \"Marketing\"),\n",
    "                    (103, \"Finance\"),\n",
    "                    (104, \"IT\"),\n",
    "                    (105, \"Support\")\n",
    "                    ]\n",
    "columns_departments = [\"dept_id\", \"dept_name\"]\n",
    "df_departments = spark.createDataFrame(data_departments, columns_departments)\n",
    "# Perform a broadcast join\n",
    "# The goal is to join these datasets on the department ID without causing a large shuffle of the departments dataset.\n",
    "df_joined = df_employees.join(broadcast(df_departments), \"dept_id\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Partition Tuning: Repartition to Increase Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition a DataFrame to increase or decrease the number of partitions, improving parallelism or reducing shuffle costs.\n",
    "#But it may still trigger a full shuffle\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "  (1, 'foo'), (2, 'bar'), (3, 'baz'), (4, 'qux')\n",
    "], [\"id\", \"value\"])\n",
    "df_repartitioned = df.repartition(10)  # Increasing the number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partition Tuning: Coalesce to Reduce Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Avoiding Full Shuffle** : coalesce is optimal when you need to reduce the number of partitions after filtering down a large dataset, and you want to avoid the cost of shuffling.\n",
    "+ **Typical Usage**: This method is often used after filtering a large DataFrame, where many partitions might end up being partially filled or empty. coalesce helps in managing resources more efficiently without the costly network overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of coalesce to reduce partitions without extensive shuffling\n",
    "df_filtered = df.filter(\"id > 1\")\n",
    "df_coalesced = df_filtered.coalesce(2)  # Reducing the number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Minimize data shuffling by optimizing Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Minimizing shuffling in Apache Spark through optimized transformations is a crucial aspect of enhancing the performance of Spark applications.\n",
    "+ Optimizing transformations involves structuring your data processing operations to reduce unnecessary data movement across the cluster, which can be resource-intensive and slow down execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4–1. Filter Early"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply filters as early as possible in your data processing pipeline to reduce the volume of data that needs to be shuffled later in operations like joins or aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 321:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|avg(Salary)|\n",
      "+----------+-----------+\n",
      "| Marketing|     4000.0|\n",
      "|   Finance|     3900.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "data = [(\"John\", \"Finance\", 3000), (\"Jane\", \"Marketing\", 4000), (\"Joe\", \"Marketing\", 2800), (\"Jill\", \"Finance\", 3900)]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Filter early before any wide transformation\n",
    "filtered_df = df.filter(df[\"Salary\"] > 3000)\n",
    "\n",
    "# Now perform aggregation\n",
    "aggregated_df = filtered_df.groupBy(\"Department\").avg(\"Salary\")\n",
    "aggregated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4–2. Use RDD/Narrow Transformations Where Possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Narrow transformations, such as `map` and `filter`, operate on individual partitions and do not require data shuffling. Use these operations instead of wide transformations when possible**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 3300.0000000000005), ('Jane', 4400.0), ('Joe', 3080.0000000000005), ('Jill', 4290.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 334:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Name|     UpdatedSalary|\n",
      "+----+------------------+\n",
      "|John|3300.0000000000005|\n",
      "|Jane|            4400.0|\n",
      "| Joe|3080.0000000000005|\n",
      "|Jill|            4290.0|\n",
      "+----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use map to create a new column without causing a shuffle\n",
    "rdd = df.rdd.map(lambda x: (x.Name, x.Salary * 1.1))\n",
    "print(rdd.collect())\n",
    "updated_salaries_df = spark.createDataFrame(\n",
    "  rdd, schema=[\"Name\", \"UpdatedSalary\"]\n",
    ")\n",
    "updated_salaries_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4–3. Avoid Unnecessary Shuffles with `join` by Boardcasting join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---------+\n",
      "|DeptId|Name| id|     Dept|\n",
      "+------+----+---+---------+\n",
      "|     1|John|  1|       HR|\n",
      "|     2|Jane|  2|Marketing|\n",
      "|     1| Joe|  1|       HR|\n",
      "|     2|Jill|  2|Marketing|\n",
      "+------+----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For joins, use broadcast joins when one of the datasets is significantly smaller than the other to avoid shuffling the larger dataset.\n",
    "\n",
    "# Assuming df_small is much smaller than df_large\n",
    "df_small = spark.createDataFrame(\n",
    "  [(1, \"HR\"), (2, \"Marketing\")], [\"id\", \"Dept\"]\n",
    ")\n",
    "df_large = spark.createDataFrame(\n",
    "  [(1, \"John\"), (2, \"Jane\"), (1, \"Joe\"), (2, \"Jill\")],\n",
    "  [\"DeptId\", \"Name\"]\n",
    ")\n",
    "# Broadcast the smaller DataFrame to optimize the join\n",
    "optimized_join_df = df_large.join(broadcast(df_small), df_large.DeptId == df_small.id)\n",
    "optimized_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4–4. Repartition Strategically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have to use wide transformations, repartition data based on keys that you will join or aggregate on later. This strategy can reduce shuffling by co-locating rows with the same key on the same partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 339:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|avg(Salary)|\n",
      "+----------+-----------+\n",
      "|   Finance|     3450.0|\n",
      "| Marketing|     3400.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Repartition before an aggregation to minimize shuffling\n",
    "repartitioned_df = df.repartition(\"Department\")\n",
    "aggregated_df = repartitioned_df.groupBy(\"Department\").avg(\"Salary\")\n",
    "aggregated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Monitoring and Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.executor.memory', '4g')\n",
    "spark.conf.set('spark.driver.memory', '2g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monitoring Tasks and Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Use Spark UI to monitor the performance of tasks and stages within your application.\n",
    "+ Access the Spark UI by navigating to: http://[your-spark-driver-host]:4040\n",
    "+ **Analyzing Executor Metrics**: Monitoring the metrics for each executor can give insights into memory usage, disk spills, and garbage collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Spark to collect detailed executor metrics\n",
    "spark.conf.set(\"spark.executor.metrics.pollingInterval\", \"5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tuning SQL Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leverage the `EXPLAIN` plan to understand and optimize the SQL execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Scan ExistingRDD (1)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [3]: [Name#3615, Department#3616, Salary#3617L]\n",
      "Arguments: [Name#3615, Department#3616, Salary#3617L], MapPartitionsRDD[633] at applySchemaToPythonRDD at <unknown>:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamic Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable dynamic allocation to allow Spark to adjust the number of executors dynamically based on the workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "spark.conf.set(\"spark.dynamicAllocation.maxExecutors\", \"20\")\n",
    "spark.conf.set(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\n",
    "spark.conf.set(\"spark.shuffle.service.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize data locality by minimizing the distance data has to travel between the storage and processing units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.locality.wait\", \"300ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Garbage Collection Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the garbage collector settings to optimize memory management and reduce pause times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use G1GC for better latency\n",
    "spark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\n",
    "# Set up explicit GC settings to optimize for short pauses\n",
    "spark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:MaxGCPauseMillis=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-Tuning Data Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data serialization plays a vital role in the performance of distributed applications. Spark supports two serializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kryo serializer for better performance and efficiency\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "spark.conf.set(\"spark.kryo.registrationRequired\", \"true\")\n",
    "\n",
    "# Register custom classes with Kryo\n",
    "class MyClass:\n",
    "    def __init__(self, name, id):\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "spark.sparkContext.getConf().registerKryoClasses([MyClass])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimizing Network Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network settings can significantly impact the performance, especially in large-scale deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune network timeout settings to avoid unnecessary job failures in large clusters\n",
    "spark.conf.set(\"spark.network.timeout\", \"800s\")\n",
    "spark.conf.set(\"spark.core.connection.ack.wait.timeout\", \"600s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Spark SQL Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leveraging the Catalyst optimizer and Tungsten execution engine can enhance the performance of Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable whole stage code generation for serialized processing\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n",
    "\n",
    "# Increase the max number of bytes for broadcasting a table, \n",
    "# which is useful for join optimization\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Optimizing Data Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune the data distribution to enhance query performance and reduce shuffle overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually setting the number of shuffle partitions based on data size and operation\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\") \n",
    "# Adjust based on your cluster size and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Enabling Adaptive Query Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Query Execution (AQE) is a feature that makes Spark SQL queries faster and more robust to data skew and other issues by adapting query plans in runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Adaptive Query Execution, which can simplify configuration and improve performance\n",
    "spark.conf.set('spark.sql.adaptive.enabled', 'true')\n",
    "# AQE can adjust shuffle partitioning, handle skewed joins, and optimize sorts by adapting to the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Specifying Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper memory management can help prevent spillages and improve performance, especially for memory-intensive operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the memory fraction to be reserved for RDD storage\n",
    "spark.conf.set('spark.memory.fraction', '0.6')\n",
    "spark.conf.set('spark.memory.storageFraction', '0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
